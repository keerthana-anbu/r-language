#importing NLp based libary
library(udpipe)#for data prep inNLP

#importing dataset
data(brussels_listings,package='udpipe')

#viewing dataset
View(brussels_listings)

#segregating a column
x<-table(brussels_listings$neighbourhood)
View(x)

#sorting
x<-sort(x)
View(x)


#importing text visualisation library
library(textplot)#for complex relation in texts

#word frequancy bar
textplot_bar(x,panel="locations",col.panel="darkgrey",xlab="listings",
             context=0.75,addpct=TRUE,cexpct=0.5)

#importing dataset
data("brussels_reviews_anno",package='udpipe')
View(brussels_reviews_anno)

#segregating dataset
y<-subset(brussels_reviews_anno,xpos %in% "NN" & language %in% "nl" & !is.na(lemma))
View(y)

#docu,emt term frequncy
y1<-document_term_frequencies(y,document="doc_id",term="lemma")
View(y1)

#document term matrix
dtm <-document_term_matrix(y)
dtm

#removing low frequency word
dtm <-dtm_remove_lowfreq(dtm,maxterms=60)
dtm


#correlation matrix
cor<-dtm_cor(dtm)
View(cor)


#importing libraries
library(glasso)#for graphical lasso:estimation of gaussian graphical

#word correlation plot
#textplot_correlation_glasso(cor,exclude_zero=TRUE)

#word concurrence graph
#segregating data
w<- subset(brussels_reviews_anno,xpos %in% "JJ" & language %in% "fr")
View(w)


#co occuring terms
w<-cooccurrence(w,group="doc_id",term = "lemma")
View(w)

#cooccurence plot
textplot_cooccurrence(w,top_n=25,subtitle="showing only top 25")

#dependency parsing
#creating data
sentence="hi!niran ,how are you? i missed you a lot when i remember our precious moment."

#tokenize and Pos tag for each word inn data
z<-udpipe(sentence,"english")
View(z)

#importing relational data visualization library
library(ggraph)
#dependency parser plot
textplot_dependencyparser(z)

#importing required librqary
require(readtext)

#getting data
 data_mobydick <-texts(readtext(""))
 data_mobydick
 
 #renaming
 names(data_mobydick) <-"moby Dick"
 
 #lexical dispersion plot for multiple words in a single document
 
 textplot_xray(
   kwic(tokens(data_mobydick),pattern="whale"),
   kwic(tokens(data_mobydick),pattern="ahab"))
 
 #importing library
 library(quanteda.textmodels)#for scaling models and classifier for sparse
 
 #importing dataset
 data(data_corpus_irishbudget2010,package="quanteda.textmodels")
 
 #transform corpus to dfm
 dt_dfm <-dfm(tokens(data_corpus_irishbudget2010))
 dt_dfm
 
 #setting reference scores
 refscores <-c(rep(NA,4),1,-1,rep(NA,8))
 refscores
 
 #predicting word scores model
 ws<-textmodel_wordscores(dt_dfm,y=refscores,smooth=1)
 ws
 
 #plot estimated word position
textplot_scale1d(ws,highlighted=c("minister","have","our","budget"),
                 highlighted_color="red")
#prediction
pred<-predict(ws,se.fit=TRUE)
pred


#plot estimated document positions
#group by "party
textplot_scale1d(pred-1bg,margin="documents",
                 groups=docvars(data_corpus_irishbudget2010,"party"))

#estimate wordfish model
wf <-textmodel_wordfish(dfm(tokens(data_corpus_irishbudget2010)),dir=c(6,5))
wf

#plot estimated word positions based on wordfish model
textplot_scale1d(wf,margin="features",
                 highlighted=c("government","global","children",
                               "bank","economy","the","citizenship",
                               "productivity","deficit"),highlighted_color="red")

#plot estimated document positions based on wordfish model
textplot_scale1d(wf,groups=data_corpus_irishbudget2010$party)


ca<-textmodel_ca(dt_dfm)
ca

#summary
summary(ca)


textplot_scale1d(ca,margin="documents",
                 groups=docvars(data_corpus_irishbudget2010,"party"))



